# T5 

## ğŸ”¹ Nom du modÃ¨le
T5 â€“ Text-To-Text Transfer Transformer

## ğŸ”¹ Type
LLM (Large Language Model)

## ğŸ”¹ Organisation / DÃ©veloppeur
Google Research â€“ Ã©quipe de Google Brain

## ğŸ”¹ Date de sortie
2019 (premiÃ¨re version), modÃ¨le open-source

## ğŸ”¹ TÃ¢ches rÃ©alisÃ©es
- RÃ©sumÃ© automatique de texte
- Traduction multilingue
- Classification de texte
- RÃ©ponse Ã  des questions
- ComplÃ©tion et gÃ©nÃ©ration de phrases
- Toute tÃ¢che NLP formulÃ©e comme un problÃ¨me "texte en entrÃ©e â†’ texte en sortie"

## ğŸ”¹ Exemples concrets d'usage
- RÃ©sumer un article de presse ou un document scientifique
- Traduire automatiquement un texte (par ex. anglais â†” franÃ§ais)
- RÃ©pondre Ã  une question Ã  partir d'un texte source
- UtilisÃ© dans Google Colab, Hugging Face Transformers, etc.

## ğŸ”¹ Mode d'accÃ¨s
- Open-source via Hugging Face
- ExÃ©cutable localement ou dans des notebooks (Colab, Jupyter...)
- ModÃ¨les de diffÃ©rentes tailles : T5-small Ã  T5-11B

## ğŸ”¹ Exemple de gÃ©nÃ©ration (Prompt + RÃ©sultat)
**Prompt :**
"summarize: Les autoencodeurs variationnels (VAE) permettent de modÃ©liser une distribution probabiliste latente Ã  partir des donnÃ©es observÃ©es. Cela permet de gÃ©nÃ©rer de nouvelles donnÃ©es similaires."

**RÃ©sultat (T5) :**
"Les VAE apprennent Ã  gÃ©nÃ©rer de nouvelles donnÃ©es en modÃ©lisant une distribution latente."

## ğŸ”¹ Lien vers dÃ©mo ou code
- [Demo Hugging Face â€“ T5](https://huggingface.co/t5-base)
- [Notebook Colab](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)
- [GitHub repo officiel](https://github.com/google-research/text-to-text-transfer-transformer)

## ğŸ”¹ Commentaires et remarques
âœ… TrÃ¨s flexible grÃ¢ce au format "texte â†’ texte"
âœ… Open-source et facile Ã  tester via Hugging Face
âœ… Supporte de nombreuses tÃ¢ches NLP
âŒ Moins performant que GPT-4 sur les tÃ¢ches ouvertes ou conversationnelles
âŒ NÃ©cessite un fine-tuning ou un prompt trÃ¨s bien rÃ©digÃ© pour des rÃ©sultats optimaux 