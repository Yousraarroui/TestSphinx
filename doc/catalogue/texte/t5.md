# T5 

## 🔹 Nom du modèle
T5 – Text-To-Text Transfer Transformer

## 🔹 Type
LLM (Large Language Model)

## 🔹 Organisation / Développeur
Google Research – équipe de Google Brain

## 🔹 Date de sortie
2019 (première version), modèle open-source

## 🔹 Tâches réalisées
- Résumé automatique de texte
- Traduction multilingue
- Classification de texte
- Réponse à des questions
- Complétion et génération de phrases
- Toute tâche NLP formulée comme un problème "texte en entrée → texte en sortie"

## 🔹 Exemples concrets d'usage
- Résumer un article de presse ou un document scientifique
- Traduire automatiquement un texte (par ex. anglais ↔ français)
- Répondre à une question à partir d'un texte source
- Utilisé dans Google Colab, Hugging Face Transformers, etc.

## 🔹 Mode d'accès
- Open-source via Hugging Face
- Exécutable localement ou dans des notebooks (Colab, Jupyter...)
- Modèles de différentes tailles : T5-small à T5-11B

## 🔹 Exemple de génération (Prompt + Résultat)
**Prompt :**
"summarize: Les autoencodeurs variationnels (VAE) permettent de modéliser une distribution probabiliste latente à partir des données observées. Cela permet de générer de nouvelles données similaires."

**Résultat (T5) :**
"Les VAE apprennent à générer de nouvelles données en modélisant une distribution latente."

## 🔹 Lien vers démo ou code
- [Demo Hugging Face – T5](https://huggingface.co/t5-base)
- [Notebook Colab](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)
- [GitHub repo officiel](https://github.com/google-research/text-to-text-transfer-transformer)

## 🔹 Commentaires et remarques
✅ Très flexible grâce au format "texte → texte"
✅ Open-source et facile à tester via Hugging Face
✅ Supporte de nombreuses tâches NLP
❌ Moins performant que GPT-4 sur les tâches ouvertes ou conversationnelles
❌ Nécessite un fine-tuning ou un prompt très bien rédigé pour des résultats optimaux 